"""
ChromaDB Vector Database Service
================================

This module provides vector database functionality using ChromaDB for storing and retrieving
document embeddings generated by Gemini.
"""

import chromadb
from chromadb.config import Settings
import numpy as np
import uuid
import json
import os
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """Container for document chunks with metadata"""
    id: str
    text: str
    embedding: np.ndarray
    metadata: Dict[str, Any]
    source_file: str
    chunk_index: int
    created_at: str

@dataclass
class SearchResult:
    """Container for search results"""
    chunk: DocumentChunk
    similarity_score: float
    distance: float

class ChromaVectorDB:
    """ChromaDB vector database service for document storage and retrieval"""
    
    def __init__(self, collection_name: str = "smartclass_documents", persist_directory: str = "./chroma_db"):
        """
        Initialize ChromaDB client and collection
        
        Args:
            collection_name: Name of the collection to store documents
            persist_directory: Directory to persist the database
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        
        # Create directory if it doesn't exist
        os.makedirs(persist_directory, exist_ok=True)
        
        # Initialize ChromaDB client
        try:
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # Get or create collection
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}  # Use cosine similarity
            )
            
            logger.info(f"✅ ChromaDB initialized. Collection: {collection_name}")
            logger.info(f"📊 Current collection size: {self.collection.count()}")
            
        except Exception as e:
            logger.error(f"❌ Failed to initialize ChromaDB: {e}")
            raise
    
    def add_document_chunks(self, chunks: List[DocumentChunk]) -> List[str]:
        """
        Add document chunks to the vector database
        
        Args:
            chunks: List of DocumentChunk objects to store
        
        Returns:
            List of IDs of stored chunks
        """
        try:
            if not chunks:
                return []
            
            # Prepare data for ChromaDB
            ids = [chunk.id for chunk in chunks]
            embeddings = [chunk.embedding.tolist() for chunk in chunks]
            documents = [chunk.text for chunk in chunks]
            metadatas = []
            
            for chunk in chunks:
                metadata = chunk.metadata.copy()
                metadata.update({
                    'source_file': chunk.source_file,
                    'chunk_index': chunk.chunk_index,
                    'created_at': chunk.created_at,
                    'text_length': len(chunk.text)
                })
                metadatas.append(metadata)
            
            # Add to collection
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            logger.info(f"✅ Added {len(chunks)} chunks to ChromaDB")
            return ids
            
        except Exception as e:
            logger.error(f"❌ Failed to add chunks to ChromaDB: {e}")
            raise
    
    def similarity_search(self, query_embedding: np.ndarray, top_k: int = 5, 
                         source_file_filter: Optional[str] = None) -> List[SearchResult]:
        """
        Search for similar documents using embedding similarity
        
        Args:
            query_embedding: Query embedding vector
            top_k: Number of top results to return
            source_file_filter: Optional filter by source file
        
        Returns:
            List of SearchResult objects
        """
        try:
            # Prepare query
            query_embedding_list = query_embedding.tolist()
            
            # Prepare filter
            where_filter = None
            if source_file_filter:
                where_filter = {"source_file": source_file_filter}
            
            # Perform search
            results = self.collection.query(
                query_embeddings=[query_embedding_list],
                n_results=top_k,
                where=where_filter,
                include=["documents", "metadatas", "distances", "embeddings"]
            )
            
            # Convert results to SearchResult objects
            search_results = []
            
            if results['ids'] and results['ids'][0]:  # Check if we have results
                for i in range(len(results['ids'][0])):
                    chunk_id = results['ids'][0][i]
                    document = results['documents'][0][i]
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    embedding = np.array(results['embeddings'][0][i], dtype=np.float32)
                    
                    # Create DocumentChunk
                    chunk = DocumentChunk(
                        id=chunk_id,
                        text=document,
                        embedding=embedding,
                        metadata=metadata,
                        source_file=metadata.get('source_file', ''),
                        chunk_index=metadata.get('chunk_index', 0),
                        created_at=metadata.get('created_at', '')
                    )
                    
                    # Calculate similarity score (cosine similarity = 1 - cosine distance)
                    similarity_score = 1 - distance
                    
                    search_results.append(SearchResult(
                        chunk=chunk,
                        similarity_score=similarity_score,
                        distance=distance
                    ))
            
            logger.info(f"🔍 Found {len(search_results)} similar chunks")
            return search_results
            
        except Exception as e:
            logger.error(f"❌ Failed to search ChromaDB: {e}")
            raise
    
    def get_chunks_by_source(self, source_file: str) -> List[DocumentChunk]:
        """
        Get all chunks from a specific source file
        
        Args:
            source_file: Name of the source file
        
        Returns:
            List of DocumentChunk objects
        """
        try:
            results = self.collection.get(
                where={"source_file": source_file},
                include=["documents", "metadatas", "embeddings"]
            )
            
            chunks = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    chunk = DocumentChunk(
                        id=results['ids'][i],
                        text=results['documents'][i],
                        embedding=np.array(results['embeddings'][i], dtype=np.float32),
                        metadata=results['metadatas'][i],
                        source_file=results['metadatas'][i].get('source_file', ''),
                        chunk_index=results['metadatas'][i].get('chunk_index', 0),
                        created_at=results['metadatas'][i].get('created_at', '')
                    )
                    chunks.append(chunk)
            
            logger.info(f"📄 Retrieved {len(chunks)} chunks from {source_file}")
            return chunks
            
        except Exception as e:
            logger.error(f"❌ Failed to get chunks by source: {e}")
            raise
    
    def delete_by_source(self, source_file: str) -> int:
        """
        Delete all chunks from a specific source file
        
        Args:
            source_file: Name of the source file
        
        Returns:
            Number of deleted chunks
        """
        try:
            # Get all chunks from the source file
            existing_chunks = self.get_chunks_by_source(source_file)
            
            if existing_chunks:
                # Delete by IDs
                chunk_ids = [chunk.id for chunk in existing_chunks]
                self.collection.delete(ids=chunk_ids)
                
                logger.info(f"🗑️ Deleted {len(chunk_ids)} chunks from {source_file}")
                return len(chunk_ids)
            
            return 0
            
        except Exception as e:
            logger.error(f"❌ Failed to delete chunks: {e}")
            raise
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the collection
        
        Returns:
            Dictionary with collection statistics
        """
        try:
            count = self.collection.count()
            
            # Get all metadata to analyze sources
            if count > 0:
                results = self.collection.get(include=["metadatas"])
                source_files = set()
                
                for metadata in results['metadatas']:
                    if 'source_file' in metadata:
                        source_files.add(metadata['source_file'])
                
                stats = {
                    'total_chunks': count,
                    'unique_sources': len(source_files),
                    'source_files': list(source_files),
                    'collection_name': self.collection_name
                }
            else:
                stats = {
                    'total_chunks': 0,
                    'unique_sources': 0,
                    'source_files': [],
                    'collection_name': self.collection_name
                }
            
            return stats
            
        except Exception as e:
            logger.error(f"❌ Failed to get collection stats: {e}")
            raise
    
    def clear_collection(self) -> bool:
        """
        Clear all data from the collection
        
        Returns:
            True if successful
        """
        try:
            # Delete the collection and recreate it
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
            logger.info("🧹 Collection cleared successfully")
            return True
            
        except Exception as e:
            logger.error(f"❌ Failed to clear collection: {e}")
            raise

# Test function
def test_chroma_db():
    """Test the ChromaDB vector database service"""
    try:
        print("🧪 Testing ChromaDB Vector Database...")
        
        # Initialize database
        db = ChromaVectorDB(collection_name="test_collection", persist_directory="./test_chroma_db")
        
        # Create test chunks
        test_chunks = []
        for i in range(3):
            chunk = DocumentChunk(
                id=str(uuid.uuid4()),
                text=f"This is test chunk {i} about machine learning and AI.",
                embedding=np.random.rand(768).astype(np.float32),  # Random embedding for testing
                metadata={"topic": "AI", "section": f"section_{i}"},
                source_file="test_document.pdf",
                chunk_index=i,
                created_at=datetime.now().isoformat()
            )
            test_chunks.append(chunk)
        
        # Add chunks
        ids = db.add_document_chunks(test_chunks)
        print(f"✅ Added chunks with IDs: {ids}")
        
        # Test similarity search
        query_embedding = np.random.rand(768).astype(np.float32)
        search_results = db.similarity_search(query_embedding, top_k=2)
        print(f"✅ Search returned {len(search_results)} results")
        
        # Test get by source
        source_chunks = db.get_chunks_by_source("test_document.pdf")
        print(f"✅ Retrieved {len(source_chunks)} chunks by source")
        
        # Test collection stats
        stats = db.get_collection_stats()
        print(f"✅ Collection stats: {stats}")
        
        # Clean up
        db.clear_collection()
        print("✅ Test collection cleared")
        
        print("🎉 All ChromaDB tests passed!")
        
    except Exception as e:
        print(f"❌ Test failed: {e}")
        raise

if __name__ == "__main__":
    test_chroma_db()